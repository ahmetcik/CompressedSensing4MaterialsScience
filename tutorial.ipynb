{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"teaser\" style=' background-position:  right center; background-size: 00px; background-repeat: no-repeat; \n",
    "    padding-top: 20px;\n",
    "    padding-right: 10px;\n",
    "    padding-bottom: 170px;\n",
    "    padding-left: 10px;\n",
    "    border-bottom: 14px double #333;\n",
    "    border-top: 14px double #333;' > \n",
    "\n",
    "   \n",
    "   <div style=\"text-align:center\">\n",
    "    <b><font size=\"6.4\">Compressed sensing for identifying materials descriptors</font></b>    \n",
    "  </div>\n",
    "    \n",
    "<p>\n",
    " created by:\n",
    " Emre Ahmetcik<sup>1</sup>, \n",
    " Angelo Ziletti<sup>1</sup>,\n",
    " Runhai Ouyang<sup> 1</sup>,\n",
    " Luca Ghiringhelli<sup> 1</sup>,\n",
    " and Matthias Scheffler<sup>1</sup> <br><br>\n",
    "   \n",
    "<sup>1</sup> Fritz Haber Institute of the Max Planck Society, Faradayweg 4-6, D-14195 Berlin, Germany <br>\n",
    "<span class=\"nomad--last-updated\" data-version=\"v1.0.0\">[Last updated: April 5, 2019]</span>\n",
    "</p>\n",
    "\n",
    "      \n",
    "<div> \n",
    "<img  style=\"float: left;\" src=\"data/Logo_MPG.png\" width=\"200\"> \n",
    "<img  style=\"float: right;\" src=\"data/Logo_NOMAD.png\" width=\"250\">\n",
    "</div>\n",
    "</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial shows how to find descriptive parameters (short formulas) to predict materials properties using compressed sensing tools. As an example we adress the prediction of the relative stability of 82 zincblende (ZB) versus rocksalt (RS) octet binary materials.\n",
    "\n",
    "The idea of using compressed sensing tools: Starting from simple physical quantities (\"building blocks\", here properties of the constituent free atoms such as orbital radii), millions (or billions) of candidate formulas are generated by applying arithmetic operations combining building blocks, for example forming sums and products of them. These candidate formulas constitute the so-called \"feature space\". Then a compressed sensing based method is used to select only a few of these formulas that explain the data, as introduced in \n",
    "<div style=\"padding: 1ex; margin-top: 1ex; margin-bottom: 1ex; border-style: dotted; border-width: 1pt; border-color: blue; border-radius: 3px;\">\n",
    "L. M. Ghiringhelli, J. Vybiral, S. V. Levchenko, C. Draxl, M. Scheffler: <span style=\"font-style: italic;\">Big Data of Materials Science: Critical Role of the Descriptor</span>,  Phys. Rev. Lett. 114, 105503 (2015) <a href=\"http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.114.105503\" target=\"_blank\">[PDF]</a>.</div>\n",
    "In this tutorial we use the Sure Independence Screening and Sparsifying  Operator  (SISSO) as proposed in\n",
    "<div style=\"padding: 1ex; margin-top: 1ex; margin-bottom: 1ex; border-style: dotted; border-width: 1pt; border-color: blue; border-radius: 3px;\">\n",
    "R. Ouyang, S. Curtarolo, E. Ahmetcik, M. Scheffler, L. M. Ghiringhelli: <span style=\"font-style: italic;\">SISSO: a compressed-sensing method for identifying the best low-dimensional descriptor in an immensity of offered candidates</span>, Phys. Rev. Materials  2, 083802 (2018) <a href=\"https://journals.aps.org/prmaterials/abstract/10.1103/PhysRevMaterials.2.083802\" target=\"_blank\">[PDF]</a> .\n",
    "</div>\n",
    "\n",
    "# Introduction to the compressed sensing methods\n",
    "\n",
    "The feature space is generated by creating a list of analytical expressions (the derived features), obtained by combining the primary features and arithmetic operations. We put all $m$ derived features into a descriptor matrix $\\mathbf{D} \\in \\mathbb{R}^{82 \\times m}$ where each column stands for a derived feature and each row for a compound. An $\\ell_0$-regularization \n",
    "\n",
    "$\\text{argmin}_{\\mathbf{c} \\in \\mathbb{R}^{m}} \\{\\|\\mathbf{P} - \\mathbf{D}\\mathbf{c}\\|^2_2 +\\lambda \\|\\mathbf{c}\\|_0\\}$\n",
    "\n",
    "determines those few feature columns which approximate a property vector $\\mathbf{P} \\in \\mathbb{R}^{82}$ (i.e RS vs. ZB energy differences) best. The subscript 0 stands for the $\\ell_0$-quasinorm, that counts the number of non-zero elements of $\\mathbf{c}$ and $\\lambda > 0$ is called the regularization parameter. Performing the $\\ell_0$-regularization becomes fast computational infeasable and often approximations (i.e. LASSO, orthogonal matching pursuit) are needed since in practice the $\\ell_0$-regularization needs to be solved combinatorial: All singletons, pairs, triplets, ... $n$-tuples (up to the selected maximum dimension of the descriptor) are listed and for each set a least-square regression is performed. The $n$-tuple that gives the lowest mean square error for the least-square regression fit is selected as the resulting $n$-dimensional descriptor.\n",
    "\n",
    "### The LASSO method\n",
    "A convex optimization problem can be introduced by the Least Absolute Shrinkage and Selection Operator (LASSO):\n",
    "\n",
    "$\\text{argmin}_{\\mathbf{c} \\in \\mathbb{R}^{m}} \\{\\|\\mathbf{P} - \\mathbf{D}\\mathbf{c}\\|^2_2 +\\lambda \\|\\mathbf{c}\\|_1\\}$.\n",
    "\n",
    "Under certain conditions it can find a good approximation to the $\\ell_0$-regularization.\n",
    "\n",
    "\n",
    "### The SISSO method\n",
    "SISSO works iteratively. In the first iteration, a number $k$ of features is collected that have the largest correlation (scalar product) with $\\mathbf{P}$. The feature with the largest correlation is simply the 1D descriptor. Next, a residual is constructed as the error made in the first iteration. A new set of $k$ features is now selected as those having the largest correlation with the residual. The 2D descriptor is the pair of features that yield the smallest fitting error upon least-square regression, among all possible pairs contained in the union of the sets selected in this and the first iteration. In each next iteration a new residual is constructed as the error made in the previous iteration, then a new set of $k$ features is extracted as those that have largest correlation with each new residual. The $n$D descriptor is the $n$-tuple of features that yield the smallest fitting error upon least square regression, among all possible $n$-tuples contained in the union of the sets obtained in each new iteration and all the previous iterations. If $k=1$ the method collapses to the so-called orthogonal matching pursuit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.linear_model import Lasso\n",
    "import scipy.stats as ss\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut\n",
    "\n",
    "from modules.sisso import SissoRegressor\n",
    "from modules.combine_features import combine_features\n",
    "from modules.viewer import show_structure, show_map\n",
    "\n",
    "# set display options for the notebook \n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to notebook directory, this is needed to run the tutorial\n",
    "os.chdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data\n",
    "Let us load the data from the file data/data.pkl into a data frame. The data was downloaded from the NOMAD archive and the NOMAD atomic data collection. It consists of RS-ZB energy differences (in eV/atom) of the 82 octet binary compounds, structure objects containing the atomic positions of the materials and properties of the atomic constituents. The following atomic features are considered:\n",
    "\n",
    "<div >\n",
    "   <ul>\n",
    "      <li>Z:  atomic number</li>\n",
    "      <li>period: period in the periodic table</li>\n",
    "      <li>IP: electron affinity</li>\n",
    "      <li>EA: ionization potential</li>      \n",
    "      <li>E_HOMO: HOMO</li>\n",
    "      <li>E_LUMO: LUMO</li>   \n",
    "      <li>r_(s, p, d): radius at the maximum of the s, p or d orbital.</li>\n",
    "   </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_pickle(\"data/data.pkl\")\n",
    "\n",
    "# print data without structure objects\n",
    "df.drop(['struc_obj_RS', 'struc_obj_ZB', 'struc_obj_min'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate 3x3x3 supercell of one example structure\n",
    "example_structure = df.loc['AgBr', 'struc_obj_RS'] * [3, 3, 3]\n",
    "show_structure(example_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the distribution of the energy differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['energy_diff'].tolist(), bins=30)\n",
    "plt.xlabel('$\\Delta E$')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()\n",
    "print('Standard deviation: %.3f eV/atom' % df['energy_diff'].values.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the SISSO method, we are able to predict the energy differences from the atomic features with an accuracy of 0.035 eV/atom. However, due to computational limit we will target an accuracy of around 0.1 eV/atom in this tutorial.  \n",
    "\n",
    "Now let us define a function get_data that uses the data frame df_data to define the target vector $\\mathbf{P}$ of energy differences and construct the desriptor matrix $\\mathbf{D}$ of combined features. The arguments selected_feature_list and allowed_operations specify which primary features and which arithmetic operations should be used to build the new derived features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(selected_feature_list, allowed_operations):\n",
    "    # add both '(A)', '(B)' to each feature\n",
    "    selected_featureAB_list = [f+A_or_B for f in selected_feature_list for A_or_B in ['(A)', '(B)']]\n",
    "    \n",
    "    # extract energy differences and selected features from df_data \n",
    "    P = df['energy_diff'].values\n",
    "    df_features = df[selected_featureAB_list]\n",
    "    \n",
    "    \n",
    "    # derive new features using allowed_operations\n",
    "    df_combined = combine_features(df=df_features, allowed_operations=allowed_operations)\n",
    "    return P, df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# selected_feature_list = ['IP', 'EA', 'E_HOMO', 'E_LUMO', 'r_s', 'r_p', 'r_d', 'Z', 'period', 'd']\n",
    "selected_feature_list = ['r_s', 'r_p']\n",
    "\n",
    "# allowed_operations = ['+', '-', '|-|', '*', '/' '^2', '^3',  'exp']\n",
    "allowed_operations = ['+']\n",
    "\n",
    "P, df_D = get_data(selected_feature_list, allowed_operations)\n",
    "\n",
    "# print derived features\n",
    "df_D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining low-dimensional descriptors with the $\\ell_0$ method\n",
    "\n",
    "<div style=\"list-style:disc; margin: 2px;padding: 10px;border: 0px;border:8px double   green; font-size:16px;padding-left: 32px;padding-right: 22px; width:89%\">\n",
    "<li> Perform an $\\ell_0$-regularization to identify the best low dimensional descriptors using the primary features.</li>\n",
    "<li> Show that non-linear functions of the primary features improve the models significantly. </li>\n",
    "<li> See that the $\\ell_0$-regularization can rapidly become computational infeasible.</li>\n",
    "</div>\n",
    "\n",
    "Our target is to find the best low dimensional descriptor for a linear model. The $\\ell_0$ regularization\n",
    "\n",
    "$\\text{argmin}_{\\mathbf{c} \\in \\mathbb{R}^{m}} \\{\\|\\mathbf{P} - \\mathbf{D}\\mathbf{c}\\|^2_2 +\\lambda \\|\\mathbf{c}\\|_0\\}$\n",
    "\n",
    "provides exactly what we want. It is defined in the follwing and solved combinatorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L0(P, D, dimension):\n",
    "    n_rows, n_columns = D.shape\n",
    "    D = np.column_stack((D, np.ones(n_rows)))\n",
    "    SE_min = np.inner(P ,P)\n",
    "    coef_min, permu_min = None, None\n",
    "    for permu in combinations(range(n_columns), dimension):\n",
    "        D_ls = D[:, permu + (-1,)]\n",
    "        coef, SE, __1, __2 = np.linalg.lstsq(D_ls, P, rcond=-1)\n",
    "        try:\n",
    "            if SE[0] < SE_min: \n",
    "                SE_min = SE[0]\n",
    "                coef_min, permu_min = coef, permu\n",
    "        except:\n",
    "            pass\n",
    "    RMSE = np.sqrt(SE_min/n_rows)\n",
    "    return RMSE, coef_min, permu_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the $\\ell_0$-regularization for different dimensions (numbers of non-zero coefficients in the model) and see the root mean square errors (RMSE) and the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature_list = ['r_s', 'r_p', 'r_d', 'EA', 'IP']\n",
    "allowed_operations = []\n",
    "\n",
    "P, df_D = get_data(selected_feature_list, allowed_operations)\n",
    "features_list = df_D.columns.tolist()\n",
    "D = df_D.values\n",
    "\n",
    "print(\"     RMSE   Best desriptor\")\n",
    "for dim in range(1,11):\n",
    "    RMSE, coefficients, selected_indices = L0(P,D,dim)\n",
    "    print('%2sD: %.5f' % (dim, RMSE), [features_list[i] for i in selected_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of performing the $\\ell_0$-regularization shows that the accuracy converges fast, e.g. we could leave out some components in the linear model without descreasing the accuracy. The second observation is that a linear model of the atomic features is not enough to describe the RS-ZB energy differences. A way out could be using non-linear machine learning models, e.g. kernel ridge regression or a neural network, instead of linear regression. Another way is to put the non-linearity into the descriptors by building algebraic combinations of the atomic features and mapping the few best of these more complex features onto the target again with a linear model. \n",
    "\n",
    "Run the following script to build larger feature spaces of more complex features and select the best 1D, 2D and 3D desriptor for a linear model using $\\ell_0$-regularization. Plot the results afterwards. How does the accuracy of the models change? How does the feature space size and the dimension of the descriptors depend on the needed time to solve the $\\ell_0$-problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_feature_list = ['r_s', 'r_p', 'r_d', 'EA', 'IP']\n",
    "op_lists = [[], ['+','|-|'], ['+','|-|','exp'], ['+','|-|','exp', '^2'] ]\n",
    "X  = []\n",
    "Errors, Time = np.empty([3,len(op_lists)]), np.empty([3,len(op_lists)])\n",
    "\n",
    "for n_op, allowed_operations in enumerate(op_lists):\n",
    "    P, df_D = get_data(selected_feature_list, allowed_operations)\n",
    "    features_list = df_D.columns.tolist()\n",
    "    D = df_D.values\n",
    "    \n",
    "    number_of_features = len(features_list)\n",
    "    X.append(number_of_features)\n",
    "    for dim in range(1,4):\n",
    "        t1= time()\n",
    "        RMSE, coefficients, selected_indices = L0(P,D,dim)\n",
    "        t2 = time()-t1             \n",
    "        \n",
    "        Time [dim-1][n_op] = t2\n",
    "        Errors[dim-1][n_op] = RMSE \n",
    "        \n",
    "        print(\"n_features: %s; %sD  RMSE: %.3f  best features: %s\" \n",
    "              %(len(features_list), dim, RMSE, [features_list[i] for i in selected_indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "f, (ax1, ax2) = plt.subplots(1,2, sharex=True, figsize=(12,8))\n",
    "ax1.set_xlabel('Feature space size')\n",
    "ax2.set_xlabel('Feature space size')\n",
    "ax1.set_ylabel('RMSE [eV/atom]')\n",
    "ax2.set_ylabel('Time [s]')\n",
    "#ax2.set_yscale('log')\n",
    "\n",
    "for dim in range(1,4):\n",
    "    ax1.plot(X, Errors[dim-1], 's-', label='%sD' %dim)\n",
    "    ax2.plot(X, Time[dim-1], 's-', label='%sD' %dim)\n",
    "ax2.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume now that we would like to include thousands or millions of (more) complex features to obtain more accurate models..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximations to the $\\ell_0$ method\n",
    "<div style=\"list-style:disc; margin: 2px;padding: 10px;border: 0px;border:8px double   green; font-size:16px;padding-left: 32px;padding-right: 22px; width:89%\">\n",
    "<li >Perform a LASSO minimization and the SISSO method.</li>\n",
    "<li >Compare the solutions with the ones from the $\\ell_0$ method.</li>\n",
    "</div>\n",
    "\n",
    "### The LASSO\n",
    "\n",
    "\n",
    "One state-of-the art approximation to the $\\ell_0$ method is the LASSO: \n",
    "\n",
    "$\\text{argmin}_{\\mathbf{c} \\in \\mathbb{R}^{m}} \\{\\|\\mathbf{P} - \\mathbf{D}\\mathbf{c}\\|^2_2 +\\lambda \\|\\mathbf{c}\\|_1\\}$.\n",
    "\n",
    "Before performing the LASSO regression we standardize the data to have mean 0 and variance 1, since otherwise the $\\ell_2$-norm of a column would affect bias its contribution to the model. <br>\n",
    "Note that we can use the LASSO also only for feature selection. We use then a least-square model with the selected features afterwards instead of the LASSO model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_fit(lam, P, D, feature_list):\n",
    "    #LASSO\n",
    "    D_standardized = ss.zscore(D)\n",
    "    lasso =  Lasso(alpha=lam)\n",
    "    lasso.fit(D_standardized, P)\n",
    "    coef =  lasso.coef_\n",
    "    \n",
    "    # get strings of selected features\n",
    "    selected_indices = coef.nonzero()[0]\n",
    "    selected_features = [feature_list[i] for i in selected_indices]\n",
    "    \n",
    "    # get RMSE of LASSO model\n",
    "    P_predict = lasso.predict(D_standardized)\n",
    "    RMSE_LASSO = np.linalg.norm(P-P_predict) / np.sqrt(82.)\n",
    "\n",
    "    #get RMSE for least-square fit\n",
    "    D_new = D[:, selected_indices]\n",
    "    D_new = np.column_stack((D_new, np.ones(82)))\n",
    "    RMSE_LS = np.sqrt(np.linalg.lstsq(D_new,P)[1][0]/82.)\n",
    "        \n",
    "    return RMSE_LASSO, RMSE_LS, coef, selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda$ regulates the sparsity of the coefficient vector of the model. Get the data and try different $\\lambda$ by adjusting the varibale lam. How good does LASSO (directly or with a least-square fit afterwards) approximate the L0-method (when the same feature space is used for both)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import Data\n",
    "selected_feature_list = ['r_s', 'r_p', 'r_d', 'EA', 'IP']\n",
    "allowed_operations = ['+','|-|','exp', '^2']\n",
    "P, df_D = get_data(selected_feature_list, allowed_operations)\n",
    "D = df_D.values\n",
    "features_list = df_D.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change lam between 0.02 and 0.34, e.g. 0.34, 0.30, 0.20, 0.13, 0.10, 0.02\n",
    "lam = 0.2\n",
    "\n",
    "RMSE_LASSO, RMSE_LS, coef, selected_features = lasso_fit(lam, P, D, features_list)\n",
    "plt.bar(range(len(coef)), np.abs(coef))\n",
    "plt.xlabel(\"Coefficient index $i$\")\n",
    "plt.ylabel(\"$|c_i|$\")\n",
    "\n",
    "print(\"lambda: %.3f\\t dimension of descriptor: %s\\t RMSE_LASSO: %.3f\\t RMSE_LS: %.3f\" \n",
    "      %(lam, len(selected_features), RMSE_LASSO, RMSE_LS))\n",
    "print(pd.DataFrame({'features':np.array(selected_features), 'abs(nonzero_coefs_LASSO)': np.abs(coef[coef.nonzero()])}))\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint:\n",
    "Compare these results to the L0 results you have obtained before from the same feature space, copied and pasted in here:<br>\n",
    "\"Number of total features generated: 115 <br>\n",
    "features: 115; 1D  RMSE: 0.296667841349  best features: ['(r_p(A)+r_d(B))'] <br>\n",
    "features: 115; 2D  RMSE: 0.194137970112  best features: ['(r_s(B)+r_p(A))', '(r_s(B)+r_p(A))^2'] <br>\n",
    "features: 115; 3D  RMSE: 0.170545592998  best features: ['(r_s(B)+r_p(A))', '(r_s(B)+r_p(A))^2', 'exp(r_s(B)+r_p(A))']\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The SISSO method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Data\n",
    "selected_feature_list = ['r_s', 'r_p', 'r_d', 'EA', 'IP']\n",
    "allowed_operations = ['+','|-|','exp', '^2']\n",
    "P, df_D = get_data(selected_feature_list, allowed_operations)\n",
    "D = df_D.values\n",
    "features_list = df_D.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply the SISSO algorithm. How does the SISSO method compare to the LASSO and to the $\\ell_0$-regularization in terms of accuracy (again when using the same feature space)? How fast is SISSO compared to the $\\ell_0$-regularization? How does n_features_per_sis_iter (the number of features collected per sis iteration) affect the performance? Note, that for n_features_per_sis_iter=1 SISSO becomes the so-called orthogonal matching pursuit, another well-known compressed sensing method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sisso = SissoRegressor(n_nonzero_coefs=3, n_features_per_sis_iter=10)\n",
    "\n",
    "sisso.fit(D, P)\n",
    "sisso.print_models(features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the SISSO method with a big feature space\n",
    "<div style=\"list-style:disc; margin: 2px;padding: 10px;border: 0px;border:8px double   green; font-size:16px;padding-left: 32px;padding-right: 22px; width:89%\">\n",
    "<li>Reproduce the results from the <a href=\"http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.114.105503\" target=\"_blank\">reference publication</a>  by including further features.</li>\n",
    "<li>Visualize the 2D descriptors in a structure map.</li>\n",
    "<li>Experiment with different settings and investigate the influence of the input parameters on the results. (OPTIONAL)</li>\n",
    "</div>\n",
    "Note the size of the feature space, the needed time to run the code and the accuracy (using the default settings)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for feature space construction\n",
    "selected_feature_list = ['IP', 'EA', 'r_s', 'r_p','r_d']\n",
    "allowed_operations = ['+','|-|','exp','^2', '/']\n",
    "\n",
    "# get the data\n",
    "P, df_D = get_data(selected_feature_list, allowed_operations)\n",
    "D = df_D.values\n",
    "features_list = df_D.columns.tolist()\n",
    "\n",
    "sisso = SissoRegressor(n_nonzero_coefs=3, n_features_per_sis_iter=26)\n",
    "\n",
    "sisso.fit(D, P)\n",
    "sisso.print_models(features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, plot an interactive 2D structure map using the 2D descriptor. By <i>hovering</i> over a point in the plot, information regarding that system is displayed. By <i>clicking</i> a point, an interactive 3D visualization of the structure will be displayed below.\n",
    "\n",
    "The markers represent the compounds and their colors the reference energy differences. How well does the descriptor separate the compounds with respect to their crystal structure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 2d solution\n",
    "P_predict = sisso.predict(D, dim=2)\n",
    "D_selcted = df_D.values[:, sisso.l0_selected_indices[1]]\n",
    "features = df_D.columns[sisso.l0_selected_indices[1]]\n",
    "\n",
    "# plot 2D map \n",
    "show_map(df, D_selcted, P_predict, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting new materials (extrapolation)\n",
    "<div style=\"list-style:disc; margin: 2px;padding: 10px;border: 0px;border:8px double   green; font-size:16px;padding-left: 32px;padding-right: 22px; width:89%\">\n",
    "<li>Perform a leave-one-out cross-validation (LOOCV) using SISSO.</li>\n",
    "<li>Analyze the prediction accuracy and how often the same descriptor is selected.</li>\n",
    "</div>\n",
    "\n",
    "We have seen that we can fit the energy differences of materials accurately. But what about predicting the energy difference of a 'new' material (which was not included when determining the model)? We test the prediction performance via LOOCV.  In a LOOCV for each material the following procedure is performed: the selected material is excluded, the model is built on the remaining materials and the model accurcy is tested on the excluded material. This means that we need to run SISSO function 82 times. <br>\n",
    "Get the data in the next cell and run the LOOCV  one cell after. Note that running the LOOCV  could take up to ten minutes. Use the remaining two cells of this chapter to analyse the results.<br>\n",
    "How is the prediction error compared to the fitting error? How often is the same descriptor selected? Are there materials which had a outlying high/low error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "selected_feature_list = ['IP', 'EA', 'r_s', 'r_p','r_d']\n",
    "allowed_operations = ['+','|-|','exp', '^2', '/']\n",
    "\n",
    "P, df_D = get_data(selected_feature_list, allowed_operations)\n",
    "features_list = df_D.columns.tolist()\n",
    "chemical_formulas = df_D.index.tolist()\n",
    "D = df_D.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave-one-out cross-validation\n",
    "n_compounds = len(P)\n",
    "dimensions = range(1, 4)\n",
    "features_count = [[] for i in range(3)]\n",
    "P_predict = np.empty([len(dimensions), n_compounds])\n",
    "\n",
    "sisso = SissoRegressor(n_nonzero_coefs=3, n_features_per_sis_iter=10)\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "for indices_train, index_test in loo.split(P):\n",
    "    i_cv = index_test[0]\n",
    "    print('%2s) Leave out %s: Ediff_ref = %.3f eV/atom' \n",
    "          % (index_test[0]+1, chemical_formulas[i_cv], P[i_cv]))\n",
    "        \n",
    "    sisso.fit(D[indices_train], P[indices_train])\n",
    "    sisso.print_models(features_list)    \n",
    "    \n",
    "    for dim in dimensions:      \n",
    "        features = [features_list[i] for i in sisso.l0_selected_indices[dim - 1]]\n",
    "        predicted_value = sisso.predict(D[index_test], dim=dim)[0]\n",
    "        \n",
    "        features_count[dim-1].append( tuple(features) )        \n",
    "        P_predict[dim-1, i_cv] = predicted_value\n",
    "        \n",
    "        print('Ediff_predicted(%sD) = %.3f eV/atom' %(dim, predicted_value))\n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Prediction errors\n",
    "prediction_errors = np.linalg.norm(P-P_predict, axis=1)/np.sqrt(n_compounds)\n",
    "for dim in dimensions:\n",
    "    predict = P_predict[dim-1]\n",
    "    if dim == 1:\n",
    "        maxi = max(max(P), max(predict))\n",
    "        mini = min(min(P), min(predict))\n",
    "        plt.plot([maxi,mini], [maxi,mini], 'k')\n",
    "    plt.scatter(P, predict, color=['b','r', 'g'][dim-1], label='%sD, RMSE = %.3f eV/atom' %(dim, prediction_errors[dim-1]))\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('E_diff_DFT'), plt.ylabel('E_diff_predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print descriptor selection frequency\n",
    "print(\"Descriptor selection frequency\")\n",
    "for dim in dimensions:    \n",
    "    df_frequency = pd.DataFrame( Counter(features_count[dim-1]).most_common(10), columns=['Features', 'Frequency'] )\n",
    "    print('-----------------\\n%sD:\\n%s' % (dim, df_frequency))\n",
    "\n",
    "# create table to display errors and models\n",
    "feat = np.array(features_count).flatten('F')\n",
    "Pred = np.array(P_predict).flatten('F')\n",
    "Pred_errors = np.abs(P-P_predict).flatten('F')\n",
    "Ref_values = [r for p in P for r in [p,p,p] ]\n",
    "Mats = [m for mat in chemical_formulas for m in [mat, mat, mat] ]\n",
    "Dims = ['1D','2D','3D'] * n_compounds\n",
    "\n",
    "df_loo = pd.DataFrame(zip(Ref_values,Pred,Pred_errors,feat), index = [Mats,Dims],\n",
    "                  columns=['P_ref[eV]', 'P_pred[eV]', 'abs. error [eV]', 'Selected features'])\n",
    "\n",
    "#  if you do not want to sort the data frame by the prediction error comment out the nex line \n",
    "df_loo = df_loo.sort_values('abs. error [eV]', ascending=False)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "display(df_loo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel ridge regession\n",
    "Last but not least, compare the prediction (LOOCV) performance of SISSO to the one of kernel ridge regression with a gaussian kernel:\n",
    "\n",
    "$K(x, y) = \\exp(-\\gamma ||x-y||^2)$.\n",
    "\n",
    "At each LOOCV step, the hyperparameters ($\\ell_2$-regularization parameter $\\lambda$ and inverse gaussian width $\\gamma$) are selected via a grid search and a so-called 5-fold cross-validation on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature_list = ['IP', 'EA', 'r_s', 'r_p','r_d']\n",
    "allowed_operations = []\n",
    "\n",
    "P, df_D = get_data(selected_feature_list, allowed_operations)\n",
    "features_list = df_D.columns.tolist()\n",
    "D = df_D.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kr = GridSearchCV(KernelRidge(kernel='rbf'), cv=5,\n",
    "                  param_grid={\"alpha\": np.logspace(-3, 0, 5),\n",
    "                              \"gamma\": np.logspace(-2, 1, 5)})\n",
    "P_predict_kr = []\n",
    "loo = LeaveOneOut()\n",
    "for indices_train, index_test in loo.split(P):\n",
    "    kr.fit(D[indices_train], P[indices_train])\n",
    "    print(\"%2i Ediff_ref: %.3f, Ediff_pred: %.3f, hyperparameters: {'lambda': %.3f, 'gamma':%.3f}\" \n",
    "          % (index_test[0], P[index_test], kr.predict(D[index_test]), \n",
    "          kr.best_params_['alpha'], kr.best_params_['gamma']))\n",
    "    P_predict_kr.append(kr.predict(D[index_test])[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_rmse_kr = np.linalg.norm(np.array(P_predict_kr) - P)/np.sqrt(P.size)\n",
    "\n",
    "maxi = max(max(P), max(P_predict_kr))\n",
    "mini = min(min(P), min(P_predict_kr))\n",
    "plt.plot([maxi,mini], [maxi,mini], 'k')\n",
    "plt.scatter(P, P_predict[-1], label='SISSO 3D, RMSE = %.3f eV/atom' % prediction_errors[dim-1])\n",
    "plt.scatter(P, P_predict_kr,  label='KR, RMSE = %.3f eV/atom' % prediction_rmse_kr)\n",
    "plt.xlabel('E_diff_DFT'), plt.ylabel('E_diff_predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
